{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e8fbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "\n",
    "trainset_dir = \"/kaggle/input/syrahq/SyRa-HQ/trainset\"\n",
    "testset_dir = \"/kaggle/input/syrahq/SyRa-HQ/testset\"\n",
    "\n",
    "target_train_A = 'derain/trainA'\n",
    "target_train_B = 'derain/trainB'\n",
    "target_test_A = 'derain/testA'\n",
    "target_test_B = 'derain/testB'\n",
    "\n",
    "max_train = 500\n",
    "max_test = 50\n",
    "\n",
    "clear_name = 'Clear.jpg'\n",
    "rain_name = 'rain_0.jpg'\n",
    "\n",
    "os.makedirs(target_train_A, exist_ok=True)\n",
    "os.makedirs(target_train_B, exist_ok=True)\n",
    "os.makedirs(target_test_A, exist_ok=True)\n",
    "os.makedirs(target_test_B, exist_ok=True)\n",
    "\n",
    "for i, folder_name in enumerate(os.listdir(trainset_dir)[:max_train*2]):\n",
    "    if i < max_train:\n",
    "        image_path = os.path.join(trainset_dir, folder_name, clear_name)\n",
    "        shutil.copy(image_path, os.path.join(target_train_A, str(i)+'.jpg'))\n",
    "    else:\n",
    "        image_path = os.path.join(trainset_dir, folder_name, rain_name)\n",
    "        shutil.copy(image_path, os.path.join(target_train_B, str(i)+'.jpg'))\n",
    "\n",
    "for i, folder_name in enumerate(os.listdir(testset_dir)[:max_test*2]):\n",
    "    if i < max_test:\n",
    "        image_path = os.path.join(testset_dir, folder_name, clear_name)\n",
    "        shutil.copy(image_path, os.path.join(target_test_A, str(i)+'.jpg'))\n",
    "    else:\n",
    "        image_path = os.path.join(testset_dir, folder_name, rain_name)\n",
    "        shutil.copy(image_path, os.path.join(target_test_B, str(i)+'.jpg'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b8affb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if CUDA is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Dataset class\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, root_A, root_B, transform=None, mode='train'):\n",
    "        self.transform = transform\n",
    "        self.files_A = sorted([os.path.join(root_A, f) for f in os.listdir(root_A)])\n",
    "        self.files_B = sorted([os.path.join(root_B, f) for f in os.listdir(root_B)])\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        img_A = Image.open(self.files_A[index % len(self.files_A)]).convert('RGB')\n",
    "        img_B = Image.open(self.files_B[index % len(self.files_B)]).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            img_A = self.transform(img_A)\n",
    "            img_B = self.transform(img_B)\n",
    "            \n",
    "        return {'A': img_A, 'B': img_B}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return max(len(self.files_A), len(self.files_B))\n",
    "\n",
    "# Generator Network (U-Net with skip connections)\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_nc=3, output_nc=3, ngf=64):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder1 = nn.Sequential(\n",
    "            nn.Conv2d(input_nc, ngf, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "        self.encoder2 = nn.Sequential(\n",
    "            nn.Conv2d(ngf, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "        self.encoder3 = nn.Sequential(\n",
    "            nn.Conv2d(ngf * 2, ngf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "        self.encoder4 = nn.Sequential(\n",
    "            nn.Conv2d(ngf * 4, ngf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Bottleneck\n",
    "        self.bottleneck = nn.Sequential(\n",
    "            nn.Conv2d(ngf * 8, ngf * 8, 4, 2, 1, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder4 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(ngf * 16, ngf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.decoder3 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.decoder2 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(ngf * 4, ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.decoder1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(ngf * 2, output_nc, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        e1 = self.encoder1(x)\n",
    "        e2 = self.encoder2(e1)\n",
    "        e3 = self.encoder3(e2)\n",
    "        e4 = self.encoder4(e3)\n",
    "        \n",
    "        # Bottleneck\n",
    "        b = self.bottleneck(e4)\n",
    "        \n",
    "        # Decoder with skip connections\n",
    "        d4 = self.decoder4(torch.cat([b, e4], 1))\n",
    "        d3 = self.decoder3(torch.cat([d4, e3], 1))\n",
    "        d2 = self.decoder2(torch.cat([d3, e2], 1))\n",
    "        d1 = self.decoder1(torch.cat([d2, e1], 1))\n",
    "        \n",
    "        return d1\n",
    "\n",
    "# PatchGAN Discriminator Network\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_nc=3, ndf=64, norm_type='batchnorm'):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        # First layer - no normalization\n",
    "        self.down1 = nn.Sequential(\n",
    "            nn.Conv2d(input_nc, ndf, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Second layer\n",
    "        if norm_type.lower() == 'batchnorm':\n",
    "            self.down2 = nn.Sequential(\n",
    "                nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "                nn.BatchNorm2d(ndf * 2),\n",
    "                nn.LeakyReLU(0.2, inplace=True)\n",
    "            )\n",
    "        else:  # instancenorm\n",
    "            self.down2 = nn.Sequential(\n",
    "                nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "                nn.InstanceNorm2d(ndf * 2),\n",
    "                nn.LeakyReLU(0.2, inplace=True)\n",
    "            )\n",
    "        \n",
    "        # Third layer\n",
    "        if norm_type.lower() == 'batchnorm':\n",
    "            self.down3 = nn.Sequential(\n",
    "                nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "                nn.BatchNorm2d(ndf * 4),\n",
    "                nn.LeakyReLU(0.2, inplace=True)\n",
    "            )\n",
    "        else:  # instancenorm\n",
    "            self.down3 = nn.Sequential(\n",
    "                nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "                nn.InstanceNorm2d(ndf * 4),\n",
    "                nn.LeakyReLU(0.2, inplace=True)\n",
    "            )\n",
    "        \n",
    "        # Fourth layer with zero padding\n",
    "        self.zero_pad1 = nn.ZeroPad2d(1)\n",
    "        self.conv = nn.Conv2d(ndf * 4, ndf * 8, 4, 1, 0, bias=False)\n",
    "        \n",
    "        if norm_type.lower() == 'batchnorm':\n",
    "            self.norm1 = nn.BatchNorm2d(ndf * 8)\n",
    "        else:  # instancenorm\n",
    "            self.norm1 = nn.InstanceNorm2d(ndf * 8)\n",
    "        \n",
    "        self.leaky_relu = nn.LeakyReLU(0.2, inplace=True)\n",
    "        \n",
    "        # Final layer\n",
    "        self.zero_pad2 = nn.ZeroPad2d(1)\n",
    "        self.last = nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=True)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, (nn.BatchNorm2d, nn.InstanceNorm2d)):\n",
    "            nn.init.normal_(m.weight, 1.0, 0.02)\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Downsample layers\n",
    "        x = self.down1(x)        # (bs, 64, 128, 128)\n",
    "        x = self.down2(x)        # (bs, 128, 64, 64)\n",
    "        x = self.down3(x)        # (bs, 256, 32, 32)\n",
    "        \n",
    "        # Fourth layer with padding\n",
    "        x = self.zero_pad1(x)    # (bs, 256, 34, 34)\n",
    "        x = self.conv(x)         # (bs, 512, 31, 31)\n",
    "        x = self.norm1(x)\n",
    "        x = self.leaky_relu(x)\n",
    "        \n",
    "        # Final layer\n",
    "        x = self.zero_pad2(x)    # (bs, 512, 33, 33)\n",
    "        x = self.last(x)         # (bs, 1, 30, 30)\n",
    "        \n",
    "        return x  # Return patch predictions (30x30 patches)\n",
    "\n",
    "# Replay Buffer for storing generated images\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, max_size=50):\n",
    "        self.max_size = max_size\n",
    "        self.data = []\n",
    "\n",
    "    def push_and_pop(self, data):\n",
    "        to_return = []\n",
    "        for element in data.data:\n",
    "            element = torch.unsqueeze(element, 0)\n",
    "            if len(self.data) < self.max_size:\n",
    "                self.data.append(element)\n",
    "                to_return.append(element)\n",
    "            else:\n",
    "                if np.random.uniform(0, 1) > 0.5:\n",
    "                    i = np.random.randint(0, self.max_size - 1)\n",
    "                    to_return.append(self.data[i].clone())\n",
    "                    self.data[i] = element\n",
    "                else:\n",
    "                    to_return.append(element)\n",
    "        return torch.cat(to_return)\n",
    "\n",
    "# Training function\n",
    "def train_cyclegan():\n",
    "    # Hyperparameters\n",
    "    IMG_SIZE = 256\n",
    "    BATCH_SIZE = 1\n",
    "    LEARNING_RATE = 0.0002\n",
    "    NUM_EPOCHS = 20\n",
    "    LAMBDA_CYCLE = 10.0\n",
    "    LAMBDA_IDENTITY = 0.5\n",
    "    \n",
    "    # Data transforms\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "    \n",
    "    # Create datasets and dataloaders\n",
    "    train_dataset = ImageDataset('derain/trainA', 'derain/trainB', transform)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "    \n",
    "    test_dataset = ImageDataset('derain/testA', 'derain/testB', transform)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "    \n",
    "    # Initialize networks\n",
    "    G_AB = Generator().to(device)  # Clear to Rain\n",
    "    G_BA = Generator().to(device)  # Rain to Clear\n",
    "    D_A = Discriminator(norm_type='batchnorm').to(device)  # Discriminator for Clear images\n",
    "    D_B = Discriminator(norm_type='batchnorm').to(device)  # Discriminator for Rain images\n",
    "    \n",
    "    # Initialize weights for generators only (discriminators handle their own)\n",
    "    def weights_init(m):\n",
    "        if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "            nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "        elif isinstance(m, nn.BatchNorm2d):\n",
    "            nn.init.normal_(m.weight, 1.0, 0.02)\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    G_AB.apply(weights_init)\n",
    "    G_BA.apply(weights_init)\n",
    "    \n",
    "    # Loss functions\n",
    "    criterion_GAN = nn.MSELoss()  # LSGAN loss for PatchGAN\n",
    "    criterion_cycle = nn.L1Loss()\n",
    "    criterion_identity = nn.L1Loss()\n",
    "    \n",
    "    # Optimizers\n",
    "    optimizer_G = optim.Adam(itertools.chain(G_AB.parameters(), G_BA.parameters()), \n",
    "                           lr=LEARNING_RATE, betas=(0.5, 0.999))\n",
    "    optimizer_D_A = optim.Adam(D_A.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))\n",
    "    optimizer_D_B = optim.Adam(D_B.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))\n",
    "    \n",
    "    # Replay buffers\n",
    "    fake_A_buffer = ReplayBuffer()\n",
    "    fake_B_buffer = ReplayBuffer()\n",
    "    \n",
    "    # Training loop - track all losses\n",
    "    G_losses = []\n",
    "    D_A_losses = []\n",
    "    D_B_losses = []\n",
    "    \n",
    "    # Individual loss tracking\n",
    "    identity_losses = []\n",
    "    gan_AB_losses = []\n",
    "    gan_BA_losses = []\n",
    "    cycle_A_losses = []\n",
    "    cycle_B_losses = []\n",
    "    \n",
    "    print(\"Starting training...\")\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        epoch_G_loss = 0\n",
    "        epoch_D_A_loss = 0\n",
    "        epoch_D_B_loss = 0\n",
    "        \n",
    "        # Individual loss accumulators\n",
    "        epoch_identity_loss = 0\n",
    "        epoch_gan_AB_loss = 0\n",
    "        epoch_gan_BA_loss = 0\n",
    "        epoch_cycle_A_loss = 0\n",
    "        epoch_cycle_B_loss = 0\n",
    "        \n",
    "        for i, batch in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS}\")):\n",
    "            real_A = batch['A'].to(device)\n",
    "            real_B = batch['B'].to(device)\n",
    "            \n",
    "            batch_size = real_A.size(0)\n",
    "            # PatchGAN outputs 30x30 patches for 256x256 input\n",
    "            real_label = torch.ones(batch_size, 1, 30, 30).to(device)\n",
    "            fake_label = torch.zeros(batch_size, 1, 30, 30).to(device)\n",
    "            \n",
    "            # ============ Train Generators ============\n",
    "            optimizer_G.zero_grad()\n",
    "            \n",
    "            # Identity loss\n",
    "            loss_id_A = criterion_identity(G_BA(real_A), real_A)\n",
    "            loss_id_B = criterion_identity(G_AB(real_B), real_B)\n",
    "            loss_identity = (loss_id_A + loss_id_B) / 2\n",
    "            \n",
    "            # GAN loss\n",
    "            fake_B = G_AB(real_A)\n",
    "            pred_fake_B = D_B(fake_B)\n",
    "            loss_GAN_AB = criterion_GAN(pred_fake_B, real_label)\n",
    "            \n",
    "            fake_A = G_BA(real_B)\n",
    "            pred_fake_A = D_A(fake_A)\n",
    "            loss_GAN_BA = criterion_GAN(pred_fake_A, real_label)\n",
    "            \n",
    "            # Cycle loss\n",
    "            recovered_A = G_BA(fake_B)\n",
    "            loss_cycle_A = criterion_cycle(recovered_A, real_A)\n",
    "            \n",
    "            recovered_B = G_AB(fake_A)\n",
    "            loss_cycle_B = criterion_cycle(recovered_B, real_B)\n",
    "            \n",
    "            # Total generator loss\n",
    "            loss_G = loss_GAN_AB + loss_GAN_BA + LAMBDA_CYCLE * (loss_cycle_A + loss_cycle_B) + LAMBDA_IDENTITY * loss_identity\n",
    "            loss_G.backward()\n",
    "            optimizer_G.step()\n",
    "            \n",
    "            # ============ Train Discriminator A ============\n",
    "            optimizer_D_A.zero_grad()\n",
    "            \n",
    "            pred_real_A = D_A(real_A)\n",
    "            loss_D_real_A = criterion_GAN(pred_real_A, real_label)\n",
    "            \n",
    "            fake_A_buffered = fake_A_buffer.push_and_pop(fake_A)\n",
    "            pred_fake_A = D_A(fake_A_buffered.detach())\n",
    "            loss_D_fake_A = criterion_GAN(pred_fake_A, fake_label)\n",
    "            \n",
    "            loss_D_A = (loss_D_real_A + loss_D_fake_A) / 2\n",
    "            loss_D_A.backward()\n",
    "            optimizer_D_A.step()\n",
    "            \n",
    "            # ============ Train Discriminator B ============\n",
    "            optimizer_D_B.zero_grad()\n",
    "            \n",
    "            pred_real_B = D_B(real_B)\n",
    "            loss_D_real_B = criterion_GAN(pred_real_B, real_label)\n",
    "            \n",
    "            fake_B_buffered = fake_B_buffer.push_and_pop(fake_B)\n",
    "            pred_fake_B = D_B(fake_B_buffered.detach())\n",
    "            loss_D_fake_B = criterion_GAN(pred_fake_B, fake_label)\n",
    "            \n",
    "            loss_D_B = (loss_D_real_B + loss_D_fake_B) / 2\n",
    "            loss_D_B.backward()\n",
    "            optimizer_D_B.step()\n",
    "            \n",
    "            # Accumulate losses\n",
    "            epoch_G_loss += loss_G.item()\n",
    "            epoch_D_A_loss += loss_D_A.item()\n",
    "            epoch_D_B_loss += loss_D_B.item()\n",
    "            \n",
    "            # Accumulate individual losses\n",
    "            epoch_identity_loss += (LAMBDA_IDENTITY * loss_identity.item())\n",
    "            epoch_gan_AB_loss += loss_GAN_AB.item()\n",
    "            epoch_gan_BA_loss += loss_GAN_BA.item()\n",
    "            epoch_cycle_A_loss += (LAMBDA_CYCLE * loss_cycle_A.item())\n",
    "            epoch_cycle_B_loss += (LAMBDA_CYCLE * loss_cycle_B.item())\n",
    "        \n",
    "        # Store epoch losses\n",
    "        G_losses.append(epoch_G_loss / len(train_loader))\n",
    "        D_A_losses.append(epoch_D_A_loss / len(train_loader))\n",
    "        D_B_losses.append(epoch_D_B_loss / len(train_loader))\n",
    "        \n",
    "        # Store individual losses\n",
    "        identity_losses.append(epoch_identity_loss / len(train_loader))\n",
    "        gan_AB_losses.append(epoch_gan_AB_loss / len(train_loader))\n",
    "        gan_BA_losses.append(epoch_gan_BA_loss / len(train_loader))\n",
    "        cycle_A_losses.append(epoch_cycle_A_loss / len(train_loader))\n",
    "        cycle_B_losses.append(epoch_cycle_B_loss / len(train_loader))\n",
    "        \n",
    "        # Print losses with more detail\n",
    "        if (epoch + 1) % 1 == 0:  # Print every 5 epochs\n",
    "            print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}]:\")\n",
    "            print(f\"  Total G_loss: {G_losses[-1]:.4f}\")\n",
    "            print(f\"    ├─ Identity: {identity_losses[-1]:.4f}\")\n",
    "            print(f\"    ├─ GAN_AB: {gan_AB_losses[-1]:.4f}\")\n",
    "            print(f\"    ├─ GAN_BA: {gan_BA_losses[-1]:.4f}\")\n",
    "            print(f\"    ├─ Cycle_A: {cycle_A_losses[-1]:.4f}\")\n",
    "            print(f\"    └─ Cycle_B: {cycle_B_losses[-1]:.4f}\")\n",
    "            print(f\"  D_A_loss: {D_A_losses[-1]:.4f}\")\n",
    "            print(f\"  D_B_loss: {D_B_losses[-1]:.4f}\")\n",
    "            print(\"-\" * 50)\n",
    "        \n",
    "        # Save sample images every 10 epochs\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            save_sample_images(G_AB, G_BA, test_loader, epoch + 1, device)\n",
    "    \n",
    "    # Save models\n",
    "    torch.save(G_AB.state_dict(), 'G_AB.pth')\n",
    "    torch.save(G_BA.state_dict(), 'G_BA.pth')\n",
    "    torch.save(D_A.state_dict(), 'D_A.pth')\n",
    "    torch.save(D_B.state_dict(), 'D_B.pth')\n",
    "    \n",
    "    # Plot comprehensive training losses\n",
    "    plt.figure(figsize=(20, 12))\n",
    "    \n",
    "    # Main losses plot\n",
    "    plt.subplot(2, 3, 1)\n",
    "    plt.plot(G_losses, label='Generator Loss', linewidth=2)\n",
    "    plt.plot(D_A_losses, label='Discriminator A Loss', linewidth=2)\n",
    "    plt.plot(D_B_losses, label='Discriminator B Loss', linewidth=2)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Main Training Losses')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # GAN losses\n",
    "    plt.subplot(2, 3, 2)\n",
    "    plt.plot(gan_AB_losses, label='GAN AB (Clear→Rain)', linewidth=2, color='blue')\n",
    "    plt.plot(gan_BA_losses, label='GAN BA (Rain→Clear)', linewidth=2, color='red')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Adversarial Losses')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Cycle losses\n",
    "    plt.subplot(2, 3, 3)\n",
    "    plt.plot(cycle_A_losses, label='Cycle A (Clear→Rain→Clear)', linewidth=2, color='green')\n",
    "    plt.plot(cycle_B_losses, label='Cycle B (Rain→Clear→Rain)', linewidth=2, color='orange')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Cycle Consistency Losses')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Identity loss\n",
    "    plt.subplot(2, 3, 4)\n",
    "    plt.plot(identity_losses, label='Identity Loss', linewidth=2, color='purple')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Identity Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Generator components breakdown\n",
    "    plt.subplot(2, 3, 5)\n",
    "    plt.plot(gan_AB_losses, label='GAN AB', alpha=0.7)\n",
    "    plt.plot(gan_BA_losses, label='GAN BA', alpha=0.7)\n",
    "    plt.plot(cycle_A_losses, label='Cycle A (×10)', alpha=0.7)\n",
    "    plt.plot(cycle_B_losses, label='Cycle B (×10)', alpha=0.7)\n",
    "    plt.plot(identity_losses, label='Identity (×0.5)', alpha=0.7)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Generator Loss Components')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Total loss comparison\n",
    "    plt.subplot(2, 3, 6)\n",
    "    total_discriminator = [(d_a + d_b) / 2 for d_a, d_b in zip(D_A_losses, D_B_losses)]\n",
    "    plt.plot(G_losses, label='Total Generator', linewidth=2, color='blue')\n",
    "    plt.plot(total_discriminator, label='Average Discriminator', linewidth=2, color='red')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Generator vs Discriminator')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle('CycleGAN Training Losses - Comprehensive View', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('comprehensive_training_losses.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Print final loss summary\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"FINAL TRAINING SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Total Generator Loss: {G_losses[-1]:.4f}\")\n",
    "    print(f\"├─ Identity Loss: {identity_losses[-1]:.4f} ({identity_losses[-1]/G_losses[-1]*100:.1f}%)\")\n",
    "    print(f\"├─ GAN AB Loss: {gan_AB_losses[-1]:.4f} ({gan_AB_losses[-1]/G_losses[-1]*100:.1f}%)\")\n",
    "    print(f\"├─ GAN BA Loss: {gan_BA_losses[-1]:.4f} ({gan_BA_losses[-1]/G_losses[-1]*100:.1f}%)\")\n",
    "    print(f\"├─ Cycle A Loss: {cycle_A_losses[-1]:.4f} ({cycle_A_losses[-1]/G_losses[-1]*100:.1f}%)\")\n",
    "    print(f\"└─ Cycle B Loss: {cycle_B_losses[-1]:.4f} ({cycle_B_losses[-1]/G_losses[-1]*100:.1f}%)\")\n",
    "    print(f\"Discriminator A Loss: {D_A_losses[-1]:.4f}\")\n",
    "    print(f\"Discriminator B Loss: {D_B_losses[-1]:.4f}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    return G_AB, G_BA\n",
    "\n",
    "# Function to save sample images\n",
    "def save_sample_images(G_AB, G_BA, test_loader, epoch, device):\n",
    "    G_AB.eval()\n",
    "    G_BA.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        batch = next(iter(test_loader))\n",
    "        real_A = batch['A'].to(device)\n",
    "        real_B = batch['B'].to(device)\n",
    "        \n",
    "        fake_B = G_AB(real_A)\n",
    "        fake_A = G_BA(real_B)\n",
    "        \n",
    "        # Convert tensors to numpy for plotting\n",
    "        def tensor_to_image(tensor):\n",
    "            image = tensor.cpu().clone()\n",
    "            image = image.squeeze(0)\n",
    "            image = image * 0.5 + 0.5  # Denormalize\n",
    "            image = transforms.ToPILImage()(image)\n",
    "            return image\n",
    "        \n",
    "        # Create subplot\n",
    "        fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "        \n",
    "        images = [real_A[0], fake_B[0], real_B[0], fake_A[0]]\n",
    "        titles = ['Real Clear', 'Generated Rain', 'Real Rain', 'Generated Clear']\n",
    "        \n",
    "        for i in range(4):\n",
    "            # Top row\n",
    "            axes[0, i].imshow(tensor_to_image(images[i]))\n",
    "            axes[0, i].set_title(titles[i])\n",
    "            axes[0, i].axis('off')\n",
    "            \n",
    "            # Bottom row - cycle consistency\n",
    "            if i < 2:\n",
    "                recovered = G_BA(fake_B)\n",
    "                axes[1, i].imshow(tensor_to_image(recovered[0]))\n",
    "                axes[1, i].set_title('Recovered Clear')\n",
    "            else:\n",
    "                recovered = G_AB(fake_A)\n",
    "                axes[1, i].imshow(tensor_to_image(recovered[0]))\n",
    "                axes[1, i].set_title('Recovered Rain')\n",
    "            axes[1, i].axis('off')\n",
    "        \n",
    "        plt.suptitle(f'CycleGAN Results - Epoch {epoch}')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'sample_epoch_{epoch}.png', dpi=150, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    G_AB.train()\n",
    "    G_BA.train()\n",
    "\n",
    "# Inference function\n",
    "def inference_and_visualization(G_AB_path='G_AB.pth', G_BA_path='G_BA.pth'):\n",
    "    \"\"\"Load trained models and generate sample images\"\"\"\n",
    "    \n",
    "    # Load models\n",
    "    G_AB = Generator().to(device)\n",
    "    G_BA = Generator().to(device)\n",
    "    \n",
    "    G_AB.load_state_dict(torch.load(G_AB_path, map_location=device))\n",
    "    G_BA.load_state_dict(torch.load(G_BA_path, map_location=device))\n",
    "    \n",
    "    G_AB.eval()\n",
    "    G_BA.eval()\n",
    "    \n",
    "    # Load test data\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "    \n",
    "    test_dataset = ImageDataset('derain/testA', 'derain/testB', transform)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=True)\n",
    "    \n",
    "    # Generate and display results\n",
    "    with torch.no_grad():\n",
    "        plt.figure(figsize=(20, 12))\n",
    "        \n",
    "        for i in range(6):  # Show 6 examples\n",
    "            batch = next(iter(test_loader))\n",
    "            real_A = batch['A'].to(device)\n",
    "            real_B = batch['B'].to(device)\n",
    "            \n",
    "            fake_B = G_AB(real_A)  # Clear to Rain\n",
    "            fake_A = G_BA(real_B)  # Rain to Clear\n",
    "            \n",
    "            # Cycle consistency\n",
    "            recovered_A = G_BA(fake_B)\n",
    "            recovered_B = G_AB(fake_A)\n",
    "            \n",
    "            def tensor_to_numpy(tensor):\n",
    "                image = tensor.cpu().squeeze(0)\n",
    "                image = image * 0.5 + 0.5\n",
    "                return image.permute(1, 2, 0).numpy()\n",
    "            \n",
    "            # Plot results\n",
    "            images = [\n",
    "                tensor_to_numpy(real_A[0]),\n",
    "                tensor_to_numpy(fake_B[0]),\n",
    "                tensor_to_numpy(recovered_A[0]),\n",
    "                tensor_to_numpy(real_B[0]),\n",
    "                tensor_to_numpy(fake_A[0]),\n",
    "                tensor_to_numpy(recovered_B[0])\n",
    "            ]\n",
    "            \n",
    "            titles = ['Real Clear', 'Generated Rain', 'Recovered Clear', \n",
    "                     'Real Rain', 'Generated Clear', 'Recovered Rain']\n",
    "            \n",
    "            for j in range(6):\n",
    "                plt.subplot(6, 6, i * 6 + j + 1)\n",
    "                plt.imshow(np.clip(images[j], 0, 1))\n",
    "                if i == 0:\n",
    "                    plt.title(titles[j])\n",
    "                plt.axis('off')\n",
    "        \n",
    "        plt.suptitle('CycleGAN Inference Results: Clear ↔ Rain Translation', fontsize=16)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('inference_results.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting CycleGAN training for Clear/Rain translation...\")\n",
    "    \n",
    "    # Train the model\n",
    "    G_AB, G_BA = train_cyclegan()\n",
    "    \n",
    "    print(\"\\nTraining completed! Running inference...\")\n",
    "    \n",
    "    # Run inference\n",
    "    inference_and_visualization()\n",
    "    \n",
    "    print(\"All done! Check the generated images and training plots.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
